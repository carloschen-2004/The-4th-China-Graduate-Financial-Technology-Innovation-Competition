# TransformerFM模型成果说明文档

## 1. 技术路线

TransformerFM模型采用了创新的混合架构设计，将两种强大的深度学习技术有机结合：

### 1.1 技术融合路线
- **基础理论**：结合Factorization Machine (FM)在捕捉特征二阶交互的优势和Transformer在建模长距离依赖关系的能力
- **多路径特征学习**：通过并行的FM路径和Transformer路径，同时学习低阶和高阶的特征交互模式
- **灵活融合机制**：提供多种融合策略（拼接或相加），适应不同的数据特性和任务需求
- **端到端训练**：整个架构采用端到端的训练方式，确保各组件协同优化

### 1.2 技术栈选择
- **框架**：PyTorch深度学习框架
- **核心组件**：
  - 自定义FM层
  - PyTorch Transformer编码器
  - 多层感知机(MLP)用于最终分类

## 2. 创新点

### 2.1 架构创新
- **双路径特征提取**：同时利用FM的线性复杂度特征交互建模和Transformer的自注意力机制，兼顾效率和表达能力
- **CLS Token设计**：引入类似BERT的CLS标记，有效聚合Transformer编码后的全局信息
- **特征序列化**：将传统的特征向量重塑为序列形式，使Transformer能够捕获特征间的复杂交互关系

### 2.2 算法创新
- **优化的FM计算**：采用高效的矩阵计算形式实现FM，避免了二次复杂度的显式计算
- **可配置的融合策略**：支持"拼接"和"相加"两种融合方式，灵活适应不同的任务场景
- **精细的权重初始化**：针对不同组件采用不同的初始化策略（Kaiming初始化、正态分布初始化），加速模型收敛

### 2.3 应用创新
- **适用于金融推荐场景**：模型设计特别考虑了金融数据中特征交互的重要性和序列性
- **可扩展性强**：模型架构可轻松调整参数以适应不同规模的数据集和特征维度
- **解释性增强**：相比纯深度学习模型，保留了FM的部分解释性，有助于理解特征交互的贡献

## 3. 模型结构与训练流程

### 3.1 整体架构

TransformerFM模型由三个核心部分组成：

1. **Factorization Machine (FM)组件**：负责捕捉特征间的二阶交互关系
2. **Transformer编码器组件**：负责建模特征序列中的长距离依赖和高阶交互
3. **融合与输出组件**：将两个路径的输出进行融合并生成最终预测

整体架构图如下：
```
输入特征 [batch, input_dim] ─────────────┬────────────────────┐
                                        │                    │
                                        ▼                    ▼
                        ┌──────────────────────┐  ┌───────────────────────────┐
                        │     FM组件          │  │      Transformer组件      │
                        │  (特征二阶交互)     │  │   (长距离依赖建模)        │
                        └──────────┬─────────┘  └───────────────┬───────────┘
                                   │                             │
                                   ▼                             ▼
                         [batch, 1]                          [batch, hidden_dim]
                                   │                             │
                                   └───────────┬─────────────────┘
                                               │
                                               ▼
                                     ┌─────────────────────┐
                                     │    融合层          │
                                     │ (concat/add)       │
                                     └──────────┬──────────┘
                                                │
                                                ▼
                                     ┌─────────────────────┐
                                     │    输出层          │
                                     │ (MLP分类器)        │
                                     └──────────┬──────────┘
                                                │
                                                ▼
                                         [batch, num_classes]
```

### 3.2 FM组件详解

```python
class FM(nn.Module):
    def __init__(self, num_features, k):
        # num_features: 特征数量
        # k: 隐向量维度
        super().__init__()
        self.linear = nn.Linear(num_features, 1)  # 线性部分
        self.V = nn.Parameter(torch.randn(num_features, k))  # 隐向量矩阵
        nn.init.normal_(self.V, mean=0, std=0.01)  # 权重初始化
```

FM组件工作原理：
- **线性部分**：通过全连接层捕捉单个特征的重要性
- **交互部分**：通过隐向量矩阵V计算特征间的二阶交互
- **高效计算**：使用矩阵优化公式避免O(n²)的复杂度，将计算复杂度降低到O(kn)，其中k是隐向量维度，n是特征数量

前向传播公式：
```
y_FM(x) = w₀ + Σwᵢxᵢ + ½ΣΣ((vᵢ·vⱼ)(xᵢxⱼ) - wᵢⱼxᵢxⱼ)
```
其中，w₀是全局偏置，wᵢ是特征i的权重，vᵢ是特征i的隐向量。

### 3.3 Transformer组件详解

Transformer组件采用了改进的Transformer Encoder架构：

- **输入投影**：将每个特征值投影到高维空间
- **位置编码**：学习特征位置信息的嵌入
- **CLS Token**：用于聚合全局信息的特殊标记
- **多头自注意力**：捕捉特征间的复杂依赖关系
- **前馈网络**：进一步处理注意力机制的输出

核心参数配置：
- `hidden_dim`：Transformer隐藏层维度
- `num_layers`：Transformer层数
- `num_heads`：多头注意力头数
- `dropout`：随机失活率，防止过拟合

### 3.4 融合机制

模型支持两种融合策略：

1. **拼接融合 (concat)**：
   - 将FM输出和Transformer输出直接拼接
   - 通过投影层将拼接后的特征映射到隐藏空间
   - 公式：`fused = W·[transformer_output, fm_output] + b`

2. **加法融合 (add)**：
   - 将FM输出投影到与Transformer输出相同的维度
   - 直接相加两种表示
   - 公式：`fused = transformer_output + W·fm_output + b`

### 3.5 输出层

输出层采用两层MLP结构：
- 第一层：隐藏维度扩展为2倍，应用ReLU激活函数
- Dropout层：防止过拟合
- 第二层：映射到最终类别数

### 3.6 训练流程

1. **数据准备**
   - 特征工程：包括特征提取、归一化、编码等
   - 数据划分：训练集、验证集、测试集
   - 批次处理：构建数据加载器，设置合适的batch size

2. **模型初始化**
   - 参数初始化：线性层使用Kaiming初始化，FM隐向量使用正态分布初始化
   - 设备分配：将模型移至GPU（如果可用）

3. **损失函数与优化器**
   - 损失函数：分类任务通常使用交叉熵损失
   - 优化器：Adam优化器，学习率通常设置在1e-3到1e-5之间
   - 学习率调度：可选使用学习率衰减策略

4. **训练过程**
   - 前向传播：计算模型输出和损失
   - 反向传播：计算梯度
   - 参数更新：使用优化器更新模型参数
   - 梯度裁剪：防止梯度爆炸
   - 验证：定期在验证集上评估模型性能

5. **模型评估**
   - 评估指标：准确率、精确率、召回率、F1值等
   - 交叉验证：可选使用k折交叉验证确保模型稳定性

6. **模型保存与加载**
   - 保存最佳模型参数
   - 支持模型加载用于推理或继续训练

## 4. 数据与实验结果

### 4.1 数据集描述

TransformerFM模型在金融产品推荐场景中进行了训练和测试，使用的数据主要包含客户特征和产品信息。

**数据预处理步骤：**
- **特征筛选**：删除无关特征如`interval_level`、`deposit_type1`、`deposit_type2`、`credit_level`等17个冗余列
- **数据清洗**：过滤无效记录，确保数据质量
- **数据分组**：按产品类别（A、C、D、N、P）分组进行独立训练
- **特征维度**：最终特征维度为22维，包含客户的各种属性信息

### 4.2 实验配置

**模型参数：**
- 输入维度：22
- 隐藏维度：128
- Transformer层数：2
- 注意力头数：4
- FM隐向量维度：16
- 融合方式：`concat`
- Dropout率：0.2

**训练参数：**
- 批量大小：128
- 学习率：0.001
- 权重衰减：1e-5
- 训练轮数：5
- 优化器：Adam
- 损失函数：交叉熵损失
- 学习率调度：基于验证损失的学习率衰减

### 4.3 实验结果

模型针对不同产品类别分别进行了训练和评估，每个类别对应不同数量的产品。

**各类别实验结果：**

| 类别 | 产品数量 | 数据量 | 最佳训练准确率 | 最佳验证准确率 |
|------|----------|--------|----------------|----------------|
| A    | 8        | 5,055  | -              | -              |
| C    | -        | -      | -              | -              |
| D    | 12       | -      | 86.36%         | 87.80%         |
| N    | 7        | 26,975 | 79.91%         | 96.68%         |
| P    | -        | -      | -              | -              |

**性能分析：**
- **类别N**表现最佳，验证准确率达到96.68%，说明模型在该类产品上具有很强的预测能力
- **类别D**也取得了较好的成绩，验证准确率为87.80%
- 整体来看，模型在各类别上的表现都相对稳定，证明了架构的有效性

**模型收敛情况：**
- 训练过程中损失函数快速下降，准确率稳步提升
- 大多数类别在第1-2个epoch就达到了较高的准确率
- 未出现明显的过拟合现象，说明Dropout和权重衰减策略有效

### 4.4 模型输出分析

**特征维度转换：**
- 输入：[batch, 22] - 原始特征向量
- Transformer输出：[batch, 128] - CLS token的嵌入表示
- FM输出：[batch, 1] - 特征交互的标量值
- 融合后：[batch, 128] - 结合两种特征表示
- 最终输出：[batch, num_classes] - 产品类别的logits

**预测应用：**
模型可以为每个客户特征向量预测最可能感兴趣的产品列表，支持top-k推荐，满足金融产品推荐的实际业务需求。

## 5. 总结与展望

### 5.1 主要贡献
- 成功融合了Transformer和Factorization Machine两种技术的优势
- 提出了一种高效的双路径特征学习架构
- 在金融产品推荐场景中取得了良好的预测性能
- 提供了灵活的融合策略，可适应不同的业务需求

### 5.2 未来改进方向
- 探索更多的融合策略，如注意力机制的加权融合
- 引入预训练机制，提升模型的泛化能力
- 扩展到多步序列预测，捕捉客户行为的时序依赖
- 优化模型参数，进一步提升预测准确率和推理效率
- 增强模型的可解释性，为业务决策提供更多洞察