{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "520095a3-d0da-4b7f-a85d-6e83ad4a2c01",
   "metadata": {},
   "source": [
    "### 第一步：将特征完全相同的产品归并"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dc48dfe0-8e51-4361-b7c5-a7356100cbbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "input_file = \"../productLabels_multiSpreadsheets.xlsx\"\n",
    "output_file = \"./first_cluster.xlsx\"\n",
    "product_column = \"prod_id\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0ece0b76-08b0-486a-b741-755d7293e238",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在处理 sheet：D ...\n",
      "正在处理 sheet：C ...\n",
      "正在处理 sheet：A ...\n",
      "正在处理 sheet：N ...\n",
      "正在处理 sheet：P ...\n",
      "✅ 全部处理完成！结果已保存为：./first_cluster.xlsx\n"
     ]
    }
   ],
   "source": [
    "# 读取xlsx文件\n",
    "all_sheets = pd.read_excel(input_file, sheet_name=None)\n",
    "# 2. 创建一个空字典，用于存储每个sheet处理后的结果\n",
    "grouped_results = {}\n",
    "# 3. 逐个sheet处理\n",
    "for sheet_name, df in all_sheets.items():\n",
    "    print(f\"正在处理 sheet：{sheet_name} ...\")\n",
    "    # 检查产品名称列是否存在\n",
    "    if product_column not in df.columns:\n",
    "        print(f\"⚠️ 跳过 {sheet_name}：未找到列 '{product_column}'\")\n",
    "        continue\n",
    "    # 获取特征列（除产品列外）\n",
    "    feature_cols = [col for col in df.columns if col != product_column]\n",
    "    # 按特征列分组，收集产品列表\n",
    "    grouped = (\n",
    "        df.groupby(feature_cols, dropna=False)[product_column]\n",
    "        .apply(list)\n",
    "        .reset_index()\n",
    "        .rename(columns={product_column: \"prod_id_list\"})\n",
    "    )\n",
    "    # 调整列顺序 & 按第一个产品排序\n",
    "    cols = [\"prod_id_list\"] + [c for c in grouped.columns if c != \"prod_id_list\"]\n",
    "    grouped = grouped[cols]\n",
    "    grouped = grouped.sort_values(by=\"prod_id_list\", key=lambda x: x.str[0])\n",
    "    # 归并后形成新的第一步聚类后的id(first_cluster_id)\n",
    "    grouped[\"first_cluster_id\"] = [str(sheet_name)+ str(idx) for idx, _ in enumerate(grouped[\"prod_id_list\"])]\n",
    "    # 存入结果字典\n",
    "    grouped_results[sheet_name] = grouped\n",
    "\n",
    "# 4. 将所有结果写入新的Excel文件\n",
    "with pd.ExcelWriter(output_file) as writer:\n",
    "    for sheet_name, grouped_df in grouped_results.items():\n",
    "        grouped_df.to_excel(writer, sheet_name=sheet_name, index=False)\n",
    "\n",
    "print(f\"✅ 全部处理完成！结果已保存为：{output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97928d2f-b2b7-4ede-8c10-268a91786018",
   "metadata": {},
   "source": [
    "### 第二步：合并event_dataset和cust_dataset, 并将D、C、A、N、P数据分列在不同的sheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "73e5bdc4-1b37-481e-b094-6a430b44be68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cust_dataset shape:  (1108827, 7)\n",
      "event_dataset shape:  (338939, 9)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 读取客户文件\n",
    "cust_dataset = pd.read_csv('../cust_dataset.csv')\n",
    "cust_dataset = pd.DataFrame(cust_dataset)\n",
    "\n",
    "# 读取事件文件\n",
    "event_dataset = pd.read_csv('../event_dataset.csv')\n",
    "event_dataset = pd.DataFrame(event_dataset)\n",
    "\n",
    "print(\"cust_dataset shape: \", cust_dataset.shape)\n",
    "print(\"event_dataset shape: \", event_dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "752e56bb-8259-426c-969a-1f79235ae199",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取产品文件\n",
    "excel_file = pd.ExcelFile('../productLabels_multiSpreadsheets.xlsx', engine='openpyxl') \n",
    "all_sheets = pd.read_excel(excel_file, sheet_name=None) \n",
    "# print(\"工作表名称：\", list(all_sheets.keys()))\n",
    "D_dataset = pd.DataFrame(all_sheets['D'])\n",
    "D_dataset.columns = [col if col == 'prod_id' else f'D_{col}' for col in D_dataset.columns]\n",
    "\n",
    "C_dataset = pd.DataFrame(all_sheets['C'])\n",
    "C_dataset.columns = [col if col == 'prod_id' else f'C_{col}' for col in C_dataset.columns]\n",
    "\n",
    "A_dataset = pd.DataFrame(all_sheets['A'])\n",
    "A_dataset.columns = [col if col == 'prod_id' else f'A_{col}' for col in A_dataset.columns]\n",
    "\n",
    "N_dataset = pd.DataFrame(all_sheets['N'])\n",
    "N_dataset.columns = [col if col == 'prod_id' else f'N_{col}' for col in N_dataset.columns]\n",
    "\n",
    "P_dataset = pd.DataFrame(all_sheets['P'])\n",
    "P_dataset.columns = [col if col == 'prod_id' else f'P_{col}' for col in P_dataset.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d736f55c-bc6c-40fb-ab3b-b45a1b5e5f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据连接：我们以事件表为核心，增加了客户、产品的信息\n",
    "event_cust = pd.merge(event_dataset,cust_dataset, on='cust_no', how = 'left')\n",
    "event_cust = pd.merge(event_cust,D_dataset,on = 'prod_id',how = 'left')\n",
    "event_cust = pd.merge(event_cust,C_dataset,on = 'prod_id',how = 'left')\n",
    "event_cust = pd.merge(event_cust,N_dataset,on = 'prod_id',how = 'left')\n",
    "event_cust = pd.merge(event_cust,A_dataset,on = 'prod_id',how = 'left')\n",
    "event_cust = pd.merge(event_cust,P_dataset,on = 'prod_id',how = 'left')\n",
    "event_cust.to_csv(\"event_cust.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4dfae241-b820-4c16-9776-f9229c0f91c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\肖异芳\\AppData\\Local\\Temp\\ipykernel_23520\\1219157190.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  filtered_df['first_cluster_id'] = filtered_df['prod_id'].map(mapping)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D :\n",
      " first_cluster_id\n",
      "D0     76163\n",
      "D1     65822\n",
      "D2     43557\n",
      "D3     29790\n",
      "D4     29045\n",
      "D5      8310\n",
      "D6      1100\n",
      "D8        63\n",
      "D7        45\n",
      "D9        32\n",
      "D10       19\n",
      "D11        3\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\肖异芳\\AppData\\Local\\Temp\\ipykernel_23520\\1219157190.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  filtered_df['first_cluster_id'] = filtered_df['prod_id'].map(mapping)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C :\n",
      " first_cluster_id\n",
      "C0     18246\n",
      "C6      7292\n",
      "C2      5982\n",
      "C7      5270\n",
      "C22     3051\n",
      "C3       957\n",
      "C11      839\n",
      "C12      709\n",
      "C15      515\n",
      "C1       381\n",
      "C10      306\n",
      "C5       242\n",
      "C4       221\n",
      "C13      209\n",
      "C14      162\n",
      "C17       64\n",
      "C9        63\n",
      "C19       26\n",
      "C21       24\n",
      "C8        14\n",
      "C18       13\n",
      "C25       11\n",
      "C16       10\n",
      "C29        9\n",
      "C28        6\n",
      "C26        5\n",
      "C20        5\n",
      "C23        2\n",
      "C27        1\n",
      "C24        1\n",
      "C30        1\n",
      "Name: count, dtype: int64\n",
      "A :\n",
      " first_cluster_id\n",
      "A1    1855\n",
      "A0    1160\n",
      "A2     657\n",
      "A6     556\n",
      "A5     166\n",
      "A3      82\n",
      "A7      70\n",
      "A4      29\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\肖异芳\\AppData\\Local\\Temp\\ipykernel_23520\\1219157190.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  filtered_df['first_cluster_id'] = filtered_df['prod_id'].map(mapping)\n",
      "C:\\Users\\肖异芳\\AppData\\Local\\Temp\\ipykernel_23520\\1219157190.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  filtered_df['first_cluster_id'] = filtered_df['prod_id'].map(mapping)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N :\n",
      " first_cluster_id\n",
      "N0    21486\n",
      "N1       13\n",
      "Name: count, dtype: int64\n",
      "P :\n",
      " first_cluster_id\n",
      "P0    8788\n",
      "P2     338\n",
      "P1      97\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\肖异芳\\AppData\\Local\\Temp\\ipykernel_23520\\1219157190.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  filtered_df['first_cluster_id'] = filtered_df['prod_id'].map(mapping)\n"
     ]
    }
   ],
   "source": [
    "# 筛选 prod_id 以 \"sheet_name\" 开头的行\n",
    "for sheet_name, grouped_df in grouped_results.items():\n",
    "    filtered_df = event_cust[event_cust['prod_id'].astype(str).str.startswith(sheet_name)]\n",
    "\n",
    "    # 为快速查找，先把 grouped 转换为字典映射：prod_id → first_cluster_id\n",
    "    mapping = {}\n",
    "\n",
    "    for _, row in grouped_df.iterrows():\n",
    "        for pid in row['prod_id_list']:\n",
    "            mapping[pid] = row['first_cluster_id']\n",
    "            \n",
    "    # 在 filtered_df 中生成新列 first_cluster_id\n",
    "    filtered_df['first_cluster_id'] = filtered_df['prod_id'].map(mapping)\n",
    "    filtered_df.to_csv(\"event_cust_\"+str(sheet_name)+\".csv\")\n",
    "    print(sheet_name,\":\\n\",filtered_df[\"first_cluster_id\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "318de52b-42ca-4463-bf90-74cc64e97081",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
